{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siri-Gith1/CSCI115_Lab/blob/main/ELELEM_Lab1_Introduction_to_the_HuggingFace_API_siri.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7jrBr1SwxqLb",
      "metadata": {
        "id": "7jrBr1SwxqLb"
      },
      "source": [
        "| | |\n",
        "|:---:|:---|\n",
        "| <img src=\"https://drive.google.com/uc?export=view&id=1ezSRk_nXkvXlCmpVaDGViMn2wef6QGfj\" width=\"100\"/> |  <strong><font size=5>EL EL EM</font></strong><br><br><strong><font color=\"#A41034\" size=5>Large Language Models: From Transformer Basics to Agentic AI</font></strong>|\n",
        "\n",
        "---\n",
        "\n",
        "# **Lab 1: Introduction to the HuggingFace API**\n",
        "\n",
        "**Instructor:**  \n",
        "Pavlos Protopapas  \n",
        "\n",
        "**Teaching Team:**  \n",
        "Chris Gumb, Shivas Jayaram, Rashmi Banthia, Shibani Budhraja, Vishnu M, Anshika Gupta, Nawang Bhutia\n",
        "\n",
        "**Contributors:**\n",
        "Ignacio Becker\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Colab Prerequisite Steps"
      ],
      "metadata": {
        "id": "2Y_S38T0pA7N"
      },
      "id": "2Y_S38T0pA7N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 📝 Make a Copy to Edit\n",
        "\n",
        "This notebook is **view-only**. To edit it, follow these steps:\n",
        "\n",
        "1. Click **File** > **Save a copy in Drive**.\n",
        "2. Your own editable copy will open in a new tab.\n",
        "\n",
        "Now you can modify and run the code freely!\n",
        "\n"
      ],
      "metadata": {
        "id": "8p00kwjj8iXA"
      },
      "id": "8p00kwjj8iXA"
    },
    {
      "cell_type": "markdown",
      "id": "b-_8-4Hqfy9n",
      "metadata": {
        "id": "b-_8-4Hqfy9n"
      },
      "source": [
        "### Setting Up Your Environment in Google Colab\n",
        "\n",
        "###Why Google Colab?\n",
        "\n",
        "- Google Colab provides a cloud-based environment that allows you to write, run, and share Python code through the browser.\n",
        "- It is especially useful for machine learning and data analysis applications because it offers free access to GPUs and TPUs, making it an ideal platform for training and testing large models.\n",
        "\n",
        "###Preparing Colab\n",
        "\n",
        "- To make the most of Google Colab for this tutorial, we need to ensure that the environment is correctly set up with all the necessary libraries and configurations.\n",
        "\n",
        "###Installation Steps (Optional / If Available)\n",
        "\n",
        "- Ensure GPU Availability: First, let's make sure that your notebook is set to use a GPU, which will speed up the model operations significantly.\n",
        "\n",
        "- Go to Runtime > Change runtime type in the Colab menu.\n",
        "Select GPU from the Hardware accelerator dropdown list and click Save.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I0fbH0KTZQaH",
      "metadata": {
        "id": "I0fbH0KTZQaH"
      },
      "source": [
        "> ### Important Notice\n",
        "\n",
        "`When running this notebook on colab, you may run out of available memory (RAM). This is expected as we are running a lot of diffferent models, simply run the imports again and only run the desired/pending sections thereafter.`\n",
        "\n",
        "\n",
        "----\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xIkpHL2awgJQ",
      "metadata": {
        "id": "xIkpHL2awgJQ"
      },
      "source": [
        "## Hugging Face\n",
        "<img src=\"https://drive.google.com/uc?id=1i87oxReRQv7rLqFuZKCPeLCh2zy8RQUU\" width=\"400\" height=\"100\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vAkPVwebNUsn",
      "metadata": {
        "id": "vAkPVwebNUsn"
      },
      "source": [
        "\n",
        "In this notebook, we will use the Hugging Face API to explore a few open source language models and their performance of simple language tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "000fdb0f"
      },
      "source": [
        "By the end of this tutorial, you will have a solid understanding of how to leverage the Hugging Face Transformers library."
      ],
      "id": "000fdb0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8228f350"
      },
      "source": [
        "## Table of Contents\n",
        "1. Embeddings Recap\n",
        "2. What is Hugging Face?\n",
        "   - How does the Hugging Face API work?\n",
        "3. Exploring the Capabilities of Modern LLMs\n",
        "   - Sentiment Analysis\n",
        "   - Text Generation\n",
        "   - Question Answering\n",
        "   - Translation\n",
        "   - NER\n",
        "   - Zero-Shot Classification\n",
        "   - Summarization\n",
        "4. Understanding the model implementation in Hugging Face (BONUS)\n",
        "   - Looking at BERT\n",
        "   - Fine Tuning BERT\n",
        "5. Quick Gradio DEMO (Toy Deployment)\n",
        "6. Exercise: Cheese Review Analysis System"
      ],
      "id": "8228f350"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings Recap\n",
        "\n",
        "Embeddings are dense vector representations of words or other data entities. They are fundamental to modern NLP because\n",
        "they allow models to process text in a way that captures semantic meaning. Words with similar meanings are represented\n",
        "by vectors that are closer together in the vector space.\n",
        "\n",
        "There are two main types of embeddings:\n",
        "\n",
        "1. Static Embeddings (e.g., Word2Vec, GloVe)\n",
        "\n",
        "    * In earlier NLP models, each word was mapped to a single, fixed vector. For example, the word \"bank\" would have the exact same embedding in both \"river bank\" and \"investment bank.\"\n",
        "    * While these embeddings capture general semantic relationships (like \"king\" is to \"queen\" as \"man\" is to \"woman\"), they cannot understand context. They fail to distinguish between different meanings of the same word (polysemy).\n",
        "\n",
        "2. Contextual Embeddings (e.g., BERT, GPT)\n",
        "\n",
        "    * Transformer-based models generate embeddings that are dynamic and depend on the surrounding words in a sentence.\n",
        "    * In this paradigm, the embedding for \"bank\" in \"river bank\" would be different from its embedding in \"investment bank.\" The model's self-attention mechanism analyzes the entire sentence to produce a context-aware vector for each token.\n",
        "    * This is a major breakthrough, as it allows the model to grasp nuance, ambiguity, and the true meaning of words as they are used in a specific context. In this lab, the models we use all rely on contextual embeddings.\n"
      ],
      "metadata": {
        "id": "2u2TQjmZe20k"
      },
      "id": "2u2TQjmZe20k"
    },
    {
      "cell_type": "markdown",
      "id": "f6bResEjb1Em",
      "metadata": {
        "id": "f6bResEjb1Em"
      },
      "source": [
        "## What is Hugging Face?\n",
        "\n",
        "[`Hugging Face`](https://huggingface.co/) is a company that specializes in natural language processing (NLP) technologies. It provides one of the most popular platforms for state-of-the-art machine learning models, particularly those designed for tasks like text analysis, language understanding, and generation. Hugging Face is widely recognized for its Transformers library, which offers easy access to pre-trained models that can perform a range of NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xJS5kit-w9IH",
      "metadata": {
        "id": "xJS5kit-w9IH"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1oG1s7346pjEn_A_EOS1QT6obiAD9o050)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UdQBUgVXYIrg",
      "metadata": {
        "id": "UdQBUgVXYIrg"
      },
      "source": [
        "### How does the Hugging Face API work?\n",
        "\n",
        "HF's Transformers library provides models that are hosted on their public model hub, and most of these models can be accessed and used *without* an API key for local computations. When you use a function like `pipeline`, the library automatically downloads the specified model from the Hugging Face model hub if it's not already present on your local machine. The actual computation does not happen on Hugging Face’s servers — it runs wherever you are executing your code, whether on your own device or, as in this Colab notebook example, on Google’s servers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face Access Token\n",
        "However, it is sometimes necessary to access the [Hugging Face Hub](https://huggingface.co/docs/hub/en/index) via an API with an authentication token. This token verifies your identity and grants you permissions beyond what is available through the public pipeline interface. Common reasons to use a token include:\n",
        "\n",
        "a. Accessing private or gated models and datasets - some resources require authentication before you can download or use them.\n",
        "\n",
        "b. Downloading models for offline use - so you can run them locally without repeated online access.\n",
        "\n",
        "c. Uploading models or datasets - contributing your own work back to the Hugging Face Hub.\n",
        "\n",
        "d. Using paid services - such as API-based inference endpoints or other premium offerings that require authentication.\n",
        "\n",
        "To download models and datasets from Hugging Face, you will need to create a free account and generate an access token. Follow these steps:\n",
        "\n",
        "1.  **Sign up on Hugging Face:**\n",
        "    *   Go to [huggingface.co/join](https://huggingface.co/join).\n",
        "    *   Choose a sign-up method (email, Google, GitHub).\n",
        "    *   Fill in the required information and create your account.\n",
        "\n",
        "2.  **Create an Access Token:**\n",
        "    *   Once logged in, click on your profile picture in the top right corner.\n",
        "    *   Select **Settings**.\n",
        "    *   In the left-hand menu, click on **Access Tokens**.\n",
        "    *   Click on **New token**.\n",
        "    *   Give your token a name (e.g., `hf-colab-token`).\n",
        "    *   Choose a role (e.g., `read` is sufficient for downloading models and datasets).\n",
        "    *   Click **Generate token**.\n",
        "    *   **Copy the generated token immediately.** You will not be able to see it again after leaving the page.\n",
        "\n",
        "3.  **Add the Access Token to Colab Secrets:**\n",
        "    *   In your Google Colab notebook, click on the **🔑 (Secrets)** icon in the left sidebar.\n",
        "    *   Click on **+ New secret**.\n",
        "    *   In the **Name** field, enter `HF_TOKEN`.\n",
        "    *   In the **Value** field, paste the access token you copied from Hugging Face.\n",
        "    *   Ensure the **Notebook access** toggle is turned **ON**.\n",
        "\n",
        "Now you can access your Hugging Face token in your Colab notebook using the following code:"
      ],
      "metadata": {
        "id": "hyZSnNS5DrkT"
      },
      "id": "hyZSnNS5DrkT"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "USE_HF_TOKEN = False\n",
        "if USE_HF_TOKEN:\n",
        "    # Get the Hugging Face token from Colab secrets\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "    # You can now use this token, for example, to log in to Hugging Face\n",
        "    from huggingface_hub import login\n",
        "    login(token=hf_token)\n",
        "\n",
        "    print(\"Hugging Face token successfully loaded.\")"
      ],
      "metadata": {
        "id": "lz_UrKSbFOff"
      },
      "id": "lz_UrKSbFOff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Capabilities of Modern LLMs\n",
        "\n",
        "This section introduces key tasks that large language models can perform, giving you hands-on practice with sentiment analysis, text generation, question answering, translation, named entity recognition, and summarization."
      ],
      "metadata": {
        "id": "Xt3shH0OA2gN"
      },
      "id": "Xt3shH0OA2gN"
    },
    {
      "cell_type": "markdown",
      "id": "NrFd4tvYgFPC",
      "metadata": {
        "id": "NrFd4tvYgFPC"
      },
      "source": [
        "**Importing the transformers library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QzdzOquQgg76",
      "metadata": {
        "id": "QzdzOquQgg76"
      },
      "outputs": [],
      "source": [
        "#Colab already has transformers installed for you! ^_^\n",
        "!pip show transformers\n",
        "\n",
        "# Importing specific modules from transformers\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `pipeline`: Simplifies inference for various NLP tasks.\n",
        "- `AutoModelForSequenceClassification`: Loads models pre-trained for classification tasks.\n",
        "- `AutoTokenizer`: Handles tokenization for model input."
      ],
      "metadata": {
        "id": "tRp4b-g1v2Rr"
      },
      "id": "tRp4b-g1v2Rr"
    },
    {
      "cell_type": "markdown",
      "id": "g-3R6ESyYSwe",
      "metadata": {
        "id": "g-3R6ESyYSwe"
      },
      "source": [
        "### **1. Sentiment Analysis**\n",
        "\n",
        "Sentiment analysis is the process of determining the emotional tone behind a piece of text — for example, classifying it as *positive* or **negative**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the Pipeline Abstraction**\n",
        "\n",
        "The pipeline() is a high-level helper class in Hugging Face that simplifies model usage by:\n",
        "1. Loading the appropriate model and tokenizer\n",
        "2. Preprocessing input text (tokenization, padding, etc.)\n",
        "3. Running model inference\n",
        "4. Post-processing outputs into human-readable format\n",
        "\n",
        "Without pipeline, you would need to handle these steps manually."
      ],
      "metadata": {
        "id": "inLOyF0C4aSN"
      },
      "id": "inLOyF0C4aSN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example without Pipeline Abstraction**\n",
        "\n"
      ],
      "metadata": {
        "id": "EW-ItEZG5LLK"
      },
      "id": "EW-ItEZG5LLK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "> You can ignore the warning about the HF_TOKEN.\n",
        "You don't need one for this notebook."
      ],
      "metadata": {
        "id": "fYm4jYMJSD7U"
      },
      "id": "fYm4jYMJSD7U"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "\n",
        "text = '''\n",
        "        I tried this cheese recently and was really disappointed. The texture was oddly rubbery, and instead of the creamy\n",
        "        richness I expected, it had a bland and artificial taste. Even after pairing it with crackers and fruit, the off-putting\n",
        "        aftertaste lingered. Definitely not something I’d buy again.\n",
        "        '''\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "outputs = model(**inputs)\n",
        "predictions = torch.nn.functional.softmax(outputs.logits)\n",
        "print(\"Probability of being Positive\", predictions[0][1])"
      ],
      "metadata": {
        "id": "h5otSz8G4fmm"
      },
      "id": "h5otSz8G4fmm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are curious about how the tokenizer handles **complicated words**, check the code cell below!"
      ],
      "metadata": {
        "id": "nU_pOU6q8KdL"
      },
      "id": "nU_pOU6q8KdL"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Peek inside the tokenizer\n",
        "#example = \"antidisestablishmentarianism\"\n",
        "example = \"Protopapas\"\n",
        "tokens = tokenizer(example, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "for token in tokens['input_ids'][0]:\n",
        "  print(tokenizer.decode(token))"
      ],
      "metadata": {
        "id": "xUSgRacj8JuB"
      },
      "id": "xUSgRacj8JuB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examples with Pipeline Abstraction**"
      ],
      "metadata": {
        "id": "oCn77bJBsUoS"
      },
      "id": "oCn77bJBsUoS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: DistilBERT (fine-tuned on [SST-2](https://huggingface.co/datasets/zacharyxxxxcr/SST-2-sentiment-analysis))**\n",
        "\n",
        "DistilBERT is smaller, faster version of BERT, fine-tuned on Stanford Sentiment Treebank\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/docs/transformers/en/model_doc/distilbert)"
      ],
      "metadata": {
        "id": "PxGkZQVp8d0e"
      },
      "id": "PxGkZQVp8d0e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WzBYAT_JYO5r",
      "metadata": {
        "id": "WzBYAT_JYO5r"
      },
      "outputs": [],
      "source": [
        "text = \"This Brie is wonderfully creamy with a rich, buttery flavor — absolutely delicious!\" #@param {type:\"string\"}\n",
        "#text = \"This Brie tastes bland and has an unpleasant ammonia smell. I wouldn’t eat it again.\"\n",
        "classifier = pipeline('sentiment-analysis',\n",
        "                     model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "result = classifier(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll now take a look at another model in action."
      ],
      "metadata": {
        "id": "hx0R60Yi6UcU"
      },
      "id": "hx0R60Yi6UcU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: RoBERTa (fine-tuned on Twitter sentiment)**  \n",
        "\n",
        "More robust BERT variant, fine-tuned on Twitter data (better for informal text)\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)"
      ],
      "metadata": {
        "id": "VLqU0zvD5601"
      },
      "id": "VLqU0zvD5601"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C1uEjmsOYaCa",
      "metadata": {
        "id": "C1uEjmsOYaCa"
      },
      "outputs": [],
      "source": [
        "text = \"This Chianti Classico has a perfect balance of acidity and fruit that pairs beautifully with Parmigiano-Reggiano.\" #@param {type:\"string\"}\n",
        "# text = \"This Chianti Classico is overly tannic and harsh, completely overwhelming the delicate flavors of Parmigiano-Reggiano.\" #@param {type:\"string\"}\n",
        "classifier = pipeline('sentiment-analysis',\n",
        "                     model='cardiffnlp/twitter-roberta-base-sentiment')\n",
        "result = classifier(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we will use the pipeline abstractions from Hugging Face to implement everything."
      ],
      "metadata": {
        "id": "w-dPhrpr52TS"
      },
      "id": "w-dPhrpr52TS"
    },
    {
      "cell_type": "markdown",
      "id": "OXQZwbjRYfH4",
      "metadata": {
        "id": "OXQZwbjRYfH4"
      },
      "source": [
        "### **2. Text Generation**\n",
        "Text generation is the task of automatically producing human-like text based on a given input or prompt — for example, completing sentences, writing stories, or generating responses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KzMBjN58prCK",
      "metadata": {
        "id": "KzMBjN58prCK"
      },
      "source": [
        "Set the maximum length of the output in the next cell.\n",
        "\n",
        "**Note**: try for a value between 30 and 50 to start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NnUXbsGnpNeQ",
      "metadata": {
        "id": "NnUXbsGnpNeQ"
      },
      "outputs": [],
      "source": [
        "max_new_tokens = 50 # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: GPT-2**\n",
        "\n",
        "General-purpose text generation, trained on web text.\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/openai-community/gpt2)"
      ],
      "metadata": {
        "id": "dlEScy2-_MQt"
      },
      "id": "dlEScy2-_MQt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A8W_7G7rYpTG",
      "metadata": {
        "id": "A8W_7G7rYpTG"
      },
      "outputs": [],
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "story_sentence = \"Once upon a time there was a professor who loved cheese\" #@param {type:\"string\"}\n",
        "result = generator(story_sentence, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
        "print(result[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: Qwen2.5**\n",
        "\n",
        "\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/Qwen/Qwen2.5-0.5B)"
      ],
      "metadata": {
        "id": "cOGaHoZ6jmUA"
      },
      "id": "cOGaHoZ6jmUA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yX5WqR5lYuNA",
      "metadata": {
        "id": "yX5WqR5lYuNA"
      },
      "outputs": [],
      "source": [
        "generator = pipeline('text-generation', model='Qwen/Qwen2.5-0.5B')\n",
        "new_story_sentence = \"Once upon a time, there was a little cheese called Chester:\" #@param {type:\"string\"}\n",
        "result = generator(new_story_sentence, max_new_tokens = max_new_tokens, num_return_sequences=1)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DybQ0D0aYwP5",
      "metadata": {
        "id": "DybQ0D0aYwP5"
      },
      "source": [
        "### **3. Question Answering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AgYs-LSoizzW",
      "metadata": {
        "id": "AgYs-LSoizzW"
      },
      "source": [
        "For basic Question-Answering,\n",
        "\n",
        "We can use a model fine-tuned on the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) (Stanford Question Answering Dataset) dataset, which is a standard benchmark for QA tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XUScK01yirRH",
      "metadata": {
        "id": "XUScK01yirRH"
      },
      "outputs": [],
      "source": [
        "# Load a QA model\n",
        "qa_pipeline = pipeline(\"question-answering\")\n",
        "\n",
        "# Set context and question\n",
        "context = \"\"\"Cheese is one of the oldest processed foods, with origins going back\n",
        "thousands of years. Archaeological evidence suggests that cheese-making began\n",
        "around 8000 BCE, shortly after the domestication of sheep. The earliest cheeses\n",
        "were likely discovered accidentally, when milk was stored in containers made from\n",
        "animal stomachs, which contained rennet. This natural enzyme caused the milk to\n",
        "separate into curds and whey. From the Middle East, cheese-making spread into\n",
        "Europe, where it became an important part of diets and local cultures, eventually\n",
        "leading to the incredible diversity of cheeses we know today.\"\"\"\n",
        "\n",
        "question = \"When was cheese first made?\" # @param {type:\"string\"}\n",
        "\n",
        "# Get the answer\n",
        "answer = qa_pipeline(question=question, context=context)\n",
        "print(answer['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5xL1cssqi5Es",
      "metadata": {
        "id": "5xL1cssqi5Es"
      },
      "source": [
        "Further, we can select our choice of models too!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: BERT**\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/docs/transformers/en/model_doc/bert)"
      ],
      "metadata": {
        "id": "cPKRQ8mA-jjs"
      },
      "id": "cPKRQ8mA-jjs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lEJy6GG0YyIj",
      "metadata": {
        "id": "lEJy6GG0YyIj"
      },
      "outputs": [],
      "source": [
        "qa_pipeline = pipeline('question-answering', model='bert-base-cased')\n",
        "\n",
        "context = \"\"\"Cheese is one of the oldest processed foods, with origins going back\n",
        "thousands of years. Archaeological evidence suggests that cheese-making began\n",
        "around 8000 BCE, shortly after the domestication of sheep. The earliest cheeses\n",
        "were likely discovered accidentally, when milk was stored in containers made from\n",
        "animal stomachs, which contained rennet. This natural enzyme caused the milk to\n",
        "separate into curds and whey. From the Middle East, cheese-making spread into\n",
        "Europe, where it became an important part of diets and local cultures, eventually\n",
        "leading to the incredible diversity of cheeses we know today.\"\"\"\n",
        "\n",
        "question = \"Who is believed to have invented cheese?\" # @param {type:\"string\"}\n",
        "\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: RoBERTa**\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/docs/transformers/en/model_doc/roberta)"
      ],
      "metadata": {
        "id": "IkXcjFMl_fdm"
      },
      "id": "IkXcjFMl_fdm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7oe0GyxBY0wE",
      "metadata": {
        "id": "7oe0GyxBY0wE"
      },
      "outputs": [],
      "source": [
        "qa_pipeline = pipeline('question-answering', model='deepset/roberta-base-squad2')\n",
        "\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FuRSMhuXY45K",
      "metadata": {
        "id": "FuRSMhuXY45K"
      },
      "source": [
        "### **4. Translation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VId_X00MqFNe",
      "metadata": {
        "id": "VId_X00MqFNe"
      },
      "outputs": [],
      "source": [
        "test_sentence =\"I am thinking about making cheese\"# @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: MarianMT**\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/docs/transformers/en/model_doc/marian)"
      ],
      "metadata": {
        "id": "8bO0LpbRY0k7"
      },
      "id": "8bO0LpbRY0k7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J_h6nzAzY6v1",
      "metadata": {
        "id": "J_h6nzAzY6v1"
      },
      "outputs": [],
      "source": [
        "translator = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr')\n",
        "result = translator(test_sentence)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: T5**\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/docs/transformers/en/model_doc/t5)"
      ],
      "metadata": {
        "id": "1-9uLma1ZBJp"
      },
      "id": "1-9uLma1ZBJp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0xFzIaVY7wf",
      "metadata": {
        "id": "a0xFzIaVY7wf"
      },
      "outputs": [],
      "source": [
        "translator = pipeline('translation_en_to_de', model='t5-base')\n",
        "result = translator(test_sentence)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y_3PKo_aqWJA",
      "metadata": {
        "id": "Y_3PKo_aqWJA"
      },
      "source": [
        "**Bonus sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "khs1B36oZLA_",
      "metadata": {
        "id": "khs1B36oZLA_"
      },
      "outputs": [],
      "source": [
        "translator = pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es')\n",
        "new_sentence = \"Professor Pavlos is the best DJ\" # @param {type:\"string\"}\n",
        "result = translator(new_sentence)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb3dph09hysM",
      "metadata": {
        "id": "bb3dph09hysM"
      },
      "source": [
        "### **5. NER**\n",
        "\n",
        "Named Entity Recognition (NER) is the task of identifying and classifying key information in text — such as names of people, organizations, locations, dates, or other specific entities."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: Bert Large Cased**\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)"
      ],
      "metadata": {
        "id": "KcjUF_AVZWAk"
      },
      "id": "KcjUF_AVZWAk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DPCnkHx4h3sQ",
      "metadata": {
        "id": "DPCnkHx4h3sQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the NER pipeline\n",
        "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "# Test sentence\n",
        "ner_test_sentence = \"Parmigiano Reggiano is often enjoyed in Italy and France. Chefs like Gordon Ramsay and Jamie Oliver have praised it, while Whole Foods in New York and London frequently showcase it.\"  # @param {type:\"string\"}\n",
        "\n",
        "# Process text\n",
        "ner_results = ner_model(ner_test_sentence)\n",
        "\n",
        "# Convert results into a DataFrame for better display\n",
        "df = pd.DataFrame(ner_results)\n",
        "display(df[[\"word\", \"entity\", \"score\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: DistilBERT NER model**\n",
        "\n",
        "To learn more, click [here](https://huggingface.co/Davlan/distilbert-base-multilingual-cased-ner-hrl)"
      ],
      "metadata": {
        "id": "Z3dxUbtuZlmU"
      },
      "id": "Z3dxUbtuZlmU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L88fOJHDidLL",
      "metadata": {
        "id": "L88fOJHDidLL"
      },
      "outputs": [],
      "source": [
        "# Load multilingual NER pipeline\n",
        "ner_distilbert = pipeline(\"ner\", model=\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\n",
        "\n",
        "# Test sentence with cheese + famous entities\n",
        "new_ner_test_sentence = \"Nestlé is considering investing in Parmigiano Reggiano production, while Elon Musk tasted Gouda in Amsterdam during a Tesla event.\"  # @param {type:\"string\"}\n",
        "\n",
        "# Process example\n",
        "distilbert_ner_results = ner_distilbert(new_ner_test_sentence)\n",
        "\n",
        "# Display results nicely in a table\n",
        "df = pd.DataFrame(distilbert_ner_results)\n",
        "display(df[[\"word\", \"entity\", \"score\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Zero-Shot Classification**\n",
        "\n",
        "   Zero-shot classification is the task of classifying text into categories that the model has not been explicitly\n",
        "   trained on. You provide a piece of text and a list of candidate labels, and the model determines which label is the\n",
        "   most relevant.\n",
        "\n",
        "   **Model: DistilBERT (fine-tuned on MNLI)**\n",
        "\n",
        "   A smaller, faster version of BERT fine-tuned for Natural Language Inference, which makes it suitable for zero-shot\n",
        "   tasks.\n"
      ],
      "metadata": {
        "id": "Q59tvNoaHje9"
      },
      "id": "Q59tvNoaHje9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the zero-shot classification pipeline with a smaller model\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"typeform/distilbert-base-uncased-mnli\")\n",
        "\n",
        "# Text to classify\n",
        "text_to_classify = \"This cheese has a wonderful nutty and sweet flavor.\"\n",
        "\n",
        "# Candidate labels\n",
        "candidate_labels = ['taste', 'texture', 'aroma', 'price']\n",
        "\n",
        "# Get the classification\n",
        "result = classifier(text_to_classify, candidate_labels)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "qWzMismzHjGs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qWzMismzHjGs"
    },
    {
      "cell_type": "markdown",
      "id": "J13rqquKjXDy",
      "metadata": {
        "id": "J13rqquKjXDy"
      },
      "source": [
        "### **7. Summarization**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eYDi9tZjgJx",
      "metadata": {
        "id": "9eYDi9tZjgJx"
      },
      "source": [
        "Try to refer to the HuggingFace docs an complete this section on your own. Try to alter important parameters like `max_length` an `min_length` and assess the responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0PIEpnc4jbcR",
      "metadata": {
        "id": "0PIEpnc4jbcR"
      },
      "outputs": [],
      "source": [
        "# Load the summarization pipeline\n",
        "# TODO: Add the task and the model which you want to choose\n",
        "summarizer = pipeline(\"___\", model=\"___\")\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"\n",
        "The Parmigiano Palace, a famed example of Art Fromage architecture, is an iconic cheeseboard in New York City, located on the east side of Manhattan.\n",
        "It was the world's tastiest wheel before it was surpassed by the Gouda State Building in 1931. Originally a project of Walter Cheddar,\n",
        "the palace was crafted by master affineurs and completed in 1930. It is known for its layered rind, composed of seven radiating cheesy arches.\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Use the summarizer pipeline to generate a concise summary of the given text\n",
        "#       - Adjust max_length and min_length as needed to control output size\n",
        "#       - Here, do_sample=False ensures deterministic output instead of random sampling\n",
        "summary = ___\n",
        "print(___)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bFUoFY03kL4h",
      "metadata": {
        "id": "bFUoFY03kL4h"
      },
      "source": [
        "## **Understanding the model implementation in Hugging Face** (BONUS) 🎁\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P-zx-WlakdCi",
      "metadata": {
        "id": "P-zx-WlakdCi"
      },
      "source": [
        "### **Looking at BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccaIou6fkpd_",
      "metadata": {
        "id": "ccaIou6fkpd_"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AlpxW8dfkcr8",
      "metadata": {
        "id": "AlpxW8dfkcr8"
      },
      "outputs": [],
      "source": [
        "# Load BERT configuration\n",
        "configuration = BertConfig()\n",
        "\n",
        "# Load BERT with its predefined configuration\n",
        "bert_model = BertModel(configuration)\n",
        "\n",
        "# Print the model architecture\n",
        "print(bert_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb74c749"
      },
      "source": [
        "<div align=\"center\">\n",
        "<img src=\"https://media0.giphy.com/media/v1.Y2lkPTZjMDliOTUyd2o4bWl5NnY3dHN0NmI3bmRhejRpM2xmaTk3MWI3YjJwejcxbWR6dSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/WRQBXSCnEFJIuxktnw/200w.gif\"/>\n",
        "</div>"
      ],
      "id": "eb74c749"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Here is a simpler way to understand the output\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "html = \"\"\"\n",
        "<style>\n",
        "  .bert-wrap{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;line-height:1.55;text-align:center}\n",
        "  .bert-title{font-size:20px;font-weight:700;margin:4px 0 14px}\n",
        "  .box{border-radius:12px;padding:12px 16px;min-width:340px;max-width:720px;margin:0 auto 18px auto;text-align:left}\n",
        "  .head{font-weight:700;margin-bottom:6px}\n",
        "  .k{font-family:ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;background:#f1f3f4;padding:2px 6px;border-radius:6px;color:#202124}\n",
        "  ul{margin:6px 0 0 18px}\n",
        "  li{margin:6px 0}\n",
        "  .legend{margin-top:10px;font-size:13px;color:#444}\n",
        "  .g{background:#e6f4ea;border:2px solid #34a853}.g .head{color:#0b7c32}\n",
        "  .b{background:#e8f0fe;border:2px solid #1a73e8}.b .head{color:#1557b0}\n",
        "  .y{background:#fff7e6;border:2px solid #fbbc04}.y .head{color:#a05a00}\n",
        "  .arrow{font-size:28px;margin:10px 0}\n",
        "  .summary{margin-top:8px;font-style:italic;color:#202124}\n",
        "</style>\n",
        "\n",
        "<div class=\"bert-wrap\">\n",
        "\n",
        "  <div class=\"bert-title\">🧭 BERT (base) — Explaining the printout</div>\n",
        "\n",
        "  <div class=\"box g\">\n",
        "    <div class=\"head\">Embeddings</div>\n",
        "    <ul>\n",
        "      <li><span class=\"k\">word_embeddings: Embedding(30522, 768, padding_idx=0)</span><br/>\n",
        "          30522 = vocab size; 768 = hidden vector length. <span class=\"k\">padding_idx=0</span> reserves ID 0 for <code>[PAD]</code>.\n",
        "      </li>\n",
        "      <li><span class=\"k\">position_embeddings: Embedding(512, 768)</span><br/>\n",
        "          Absolute positions for up to 512 tokens (tells model word order).\n",
        "      </li>\n",
        "      <li><span class=\"k\">token_type_embeddings: Embedding(2, 768)</span><br/>\n",
        "          Distinguishes sentence A vs B for paired inputs.\n",
        "      </li>\n",
        "      <li><span class=\"k\">LayerNorm((768,), eps=1e-12)</span><br/>\n",
        "          Stabilizes activations with residual connections.\n",
        "      </li>\n",
        "      <li><span class=\"k\">Dropout(p=0.1)</span><br/>\n",
        "          Randomly drops 10% during training for regularization.\n",
        "      </li>\n",
        "    </ul>\n",
        "    <div class=\"summary\">➡ Converts raw tokens into dense vectors the model can use.</div>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"arrow\">⬇</div>\n",
        "\n",
        "  <div class=\"box b\">\n",
        "    <div class=\"head\">Encoder (×12 Transformer Layers)</div>\n",
        "    <ul>\n",
        "      <li><span class=\"k\">Self-Attention (q/k/v: 768→768)</span><br/>\n",
        "          Builds queries, keys, values. Multi-head (12×64 dims). <span class=\"k\">Dropout(0.1)</span> on attention weights.\n",
        "      </li>\n",
        "      <li><span class=\"k\">Attention Output: dense 768→768</span><br/>\n",
        "          Projects back; adds residual + LayerNorm.\n",
        "      </li>\n",
        "      <li><span class=\"k\">Intermediate: dense 768→3072 + GELU</span><br/>\n",
        "          Expands to 4× hidden size with GELU activation.\n",
        "      </li>\n",
        "      <li><span class=\"k\">Output: dense 3072→768</span><br/>\n",
        "          Compresses back; residual + LayerNorm again.\n",
        "      </li>\n",
        "    </ul>\n",
        "    <p><b>Mental model:</b> Each layer = Self-Attention → Add+Norm → Feed Forward → Add+Norm.</p>\n",
        "    <div class=\"summary\">➡ Processes token vectors through 12 repeated transformer layers.</div>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"arrow\">⬇</div>\n",
        "\n",
        "  <div class=\"box y\">\n",
        "    <div class=\"head\">Pooler</div>\n",
        "    <ul>\n",
        "      <li><span class=\"k\">dense 768→768 + Tanh</span><br/>\n",
        "          Takes the <code>[CLS]</code> token output → gives a single vector (used for classification heads).\n",
        "          <span class=\"muted\">Tanh squashes values between -1 and 1, making the output more stable.</span>\n",
        "      </li>\n",
        "    </ul>\n",
        "    <div class=\"summary\">➡ Produces one summary vector for the whole sequence.</div>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"legend\">\n",
        "    <b style=\"color:#0b7c32\">Green</b> = Input prep •\n",
        "    <b style=\"color:#1557b0\">Blue</b> = Transformer stack •\n",
        "    <b style=\"color:#a05a00\">Amber</b> = Sequence summary\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RTA7w747I-xr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c587eb1d-b7fa-459c-a51a-288ab4ade796"
      },
      "id": "RTA7w747I-xr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "  .bert-wrap{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;line-height:1.55;text-align:center}\n",
              "  .bert-title{font-size:20px;font-weight:700;margin:4px 0 14px}\n",
              "  .box{border-radius:12px;padding:12px 16px;min-width:340px;max-width:720px;margin:0 auto 18px auto;text-align:left}\n",
              "  .head{font-weight:700;margin-bottom:6px}\n",
              "  .k{font-family:ui-monospace, SFMono-Regular, Menlo, Consolas, monospace;background:#f1f3f4;padding:2px 6px;border-radius:6px;color:#202124}\n",
              "  ul{margin:6px 0 0 18px}\n",
              "  li{margin:6px 0}\n",
              "  .legend{margin-top:10px;font-size:13px;color:#444}\n",
              "  .g{background:#e6f4ea;border:2px solid #34a853}.g .head{color:#0b7c32}\n",
              "  .b{background:#e8f0fe;border:2px solid #1a73e8}.b .head{color:#1557b0}\n",
              "  .y{background:#fff7e6;border:2px solid #fbbc04}.y .head{color:#a05a00}\n",
              "  .arrow{font-size:28px;margin:10px 0}\n",
              "  .summary{margin-top:8px;font-style:italic;color:#202124}\n",
              "</style>\n",
              "\n",
              "<div class=\"bert-wrap\">\n",
              "\n",
              "  <div class=\"bert-title\">🧭 BERT (base) — Explaining the printout</div>\n",
              "\n",
              "  <div class=\"box g\">\n",
              "    <div class=\"head\">Embeddings</div>\n",
              "    <ul>\n",
              "      <li><span class=\"k\">word_embeddings: Embedding(30522, 768, padding_idx=0)</span><br/>\n",
              "          30522 = vocab size; 768 = hidden vector length. <span class=\"k\">padding_idx=0</span> reserves ID 0 for <code>[PAD]</code>.\n",
              "      </li>\n",
              "      <li><span class=\"k\">position_embeddings: Embedding(512, 768)</span><br/>\n",
              "          Absolute positions for up to 512 tokens (tells model word order).\n",
              "      </li>\n",
              "      <li><span class=\"k\">token_type_embeddings: Embedding(2, 768)</span><br/>\n",
              "          Distinguishes sentence A vs B for paired inputs.\n",
              "      </li>\n",
              "      <li><span class=\"k\">LayerNorm((768,), eps=1e-12)</span><br/>\n",
              "          Stabilizes activations with residual connections.\n",
              "      </li>\n",
              "      <li><span class=\"k\">Dropout(p=0.1)</span><br/>\n",
              "          Randomly drops 10% during training for regularization.\n",
              "      </li>\n",
              "    </ul>\n",
              "    <div class=\"summary\">➡ Converts raw tokens into dense vectors the model can use.</div>\n",
              "  </div>\n",
              "\n",
              "  <div class=\"arrow\">⬇</div>\n",
              "\n",
              "  <div class=\"box b\">\n",
              "    <div class=\"head\">Encoder (×12 Transformer Layers)</div>\n",
              "    <ul>\n",
              "      <li><span class=\"k\">Self-Attention (q/k/v: 768→768)</span><br/>\n",
              "          Builds queries, keys, values. Multi-head (12×64 dims). <span class=\"k\">Dropout(0.1)</span> on attention weights.\n",
              "      </li>\n",
              "      <li><span class=\"k\">Attention Output: dense 768→768</span><br/>\n",
              "          Projects back; adds residual + LayerNorm.\n",
              "      </li>\n",
              "      <li><span class=\"k\">Intermediate: dense 768→3072 + GELU</span><br/>\n",
              "          Expands to 4× hidden size with GELU activation.\n",
              "      </li>\n",
              "      <li><span class=\"k\">Output: dense 3072→768</span><br/>\n",
              "          Compresses back; residual + LayerNorm again.\n",
              "      </li>\n",
              "    </ul>\n",
              "    <p><b>Mental model:</b> Each layer = Self-Attention → Add+Norm → Feed Forward → Add+Norm.</p>\n",
              "    <div class=\"summary\">➡ Processes token vectors through 12 repeated transformer layers.</div>\n",
              "  </div>\n",
              "\n",
              "  <div class=\"arrow\">⬇</div>\n",
              "\n",
              "  <div class=\"box y\">\n",
              "    <div class=\"head\">Pooler</div>\n",
              "    <ul>\n",
              "      <li><span class=\"k\">dense 768→768 + Tanh</span><br/>\n",
              "          Takes the <code>[CLS]</code> token output → gives a single vector (used for classification heads).\n",
              "          <span class=\"muted\">Tanh squashes values between -1 and 1, making the output more stable.</span>\n",
              "      </li>\n",
              "    </ul>\n",
              "    <div class=\"summary\">➡ Produces one summary vector for the whole sequence.</div>\n",
              "  </div>\n",
              "\n",
              "  <div class=\"legend\">\n",
              "    <b style=\"color:#0b7c32\">Green</b> = Input prep •\n",
              "    <b style=\"color:#1557b0\">Blue</b> = Transformer stack •\n",
              "    <b style=\"color:#a05a00\">Amber</b> = Sequence summary\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddMIORkdkt3-",
      "metadata": {
        "id": "ddMIORkdkt3-"
      },
      "source": [
        "### **Fine-Tuning BERT**\n",
        "\n",
        "One of the strengths of BERT’s architecture is flexibility in fine-tuning:  \n",
        "\n",
        "- **Fine-tune only the head** 🧩  \n",
        "  Keep the encoder frozen and train just the task-specific head (e.g., classifier).  \n",
        "  → Fast, lightweight, and works well when you have limited data.  \n",
        "\n",
        "- **Fine-tune part of the encoder** ⚙️  \n",
        "  Unfreeze a few top layers of the encoder (closer to the output) and update them along with the head.  \n",
        "  → Balances efficiency and performance.  \n",
        "\n",
        "- **Fine-tune everything** 🚀  \n",
        "  Train both the encoder (all transformer layers) and the head end-to-end.  \n",
        "  → Requires more data and compute but usually gives the **best performance** for complex tasks.  \n",
        "\n",
        "  (We'll see 2 examples here, first we fine tune everthing and then just the head.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OlRQgVaoktHo",
      "metadata": {
        "id": "OlRQgVaoktHo"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mRMsU57hkxtU",
      "metadata": {
        "id": "mRMsU57hkxtU"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Ae_Ym4zk0b0",
      "metadata": {
        "id": "7Ae_Ym4zk0b0"
      },
      "outputs": [],
      "source": [
        "# Example: Fine-tuning the model on a custom dataset\n",
        "# This is a placeholder; in a real scenario, you would load your dataset here\n",
        "texts = [\"I love this product!\", \"I hate this product!\"]\n",
        "labels = [1, 0]  # 1 for positive, 0 for negative sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MopFud8Mk3XG",
      "metadata": {
        "id": "MopFud8Mk3XG"
      },
      "outputs": [],
      "source": [
        "# Tokenize input\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GtU6CDBqk5Js",
      "metadata": {
        "id": "GtU6CDBqk5Js"
      },
      "outputs": [],
      "source": [
        "# Custom dataset\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = SimpleDataset(encodings, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "J25xNGNA8wGI"
      },
      "id": "J25xNGNA8wGI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for 3 epochs with accuracy\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        correct += (preds == batch[\"labels\"]).sum().item()\n",
        "        total += batch[\"labels\"].size(0)\n",
        "\n",
        "        print(f\"  Loss: {loss.item():.4f}\")\n",
        "\n",
        "    acc_full = correct / total\n",
        "    print(f\"  Accuracy: {acc_full:.4f}\\n\")"
      ],
      "metadata": {
        "id": "bGvv3XsY8w-v"
      },
      "id": "bGvv3XsY8w-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to fine tune would be to freeze the model \"body\" and only train the \"head\""
      ],
      "metadata": {
        "id": "_uTrJRDEZBLc"
      },
      "id": "_uTrJRDEZBLc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload a fresh model so weights are reset\n",
        "model_frozen = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Freeze all BERT encoder layers\n",
        "for param in model_frozen.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Check trainable vs frozen parameters\n",
        "frozen, trainable = 0, 0\n",
        "for name, param in model_frozen.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable += 1\n",
        "    else:\n",
        "        frozen += 1\n",
        "print(f\"Frozen layers: {frozen}, Trainable layers: {trainable}\")"
      ],
      "metadata": {
        "id": "hFccVxetHJsN"
      },
      "id": "hFccVxetHJsN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer (now only updates the head)\n",
        "optimizer_frozen = AdamW(model_frozen.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "1-gzaHR2HwTE"
      },
      "id": "1-gzaHR2HwTE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop (same as before) for 3 epochs\n",
        "model_frozen.train()\n",
        "for epoch in range(3):\n",
        "    print(f\"[Frozen] Epoch {epoch+1}\")\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer_frozen.zero_grad()\n",
        "        outputs = model_frozen(**batch)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer_frozen.step()\n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        correct += (preds == batch[\"labels\"]).sum().item()\n",
        "        total += batch[\"labels\"].size(0)\n",
        "\n",
        "        print(f\"  Loss: {loss.item():.4f}\")\n",
        "\n",
        "    acc_frozen = correct / total\n",
        "    print(f\"  Accuracy: {acc_frozen:.4f}\\n\")"
      ],
      "metadata": {
        "id": "R1SbE_yjHLSF"
      },
      "id": "R1SbE_yjHLSF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "-FPuXFJzm16E",
      "metadata": {
        "id": "-FPuXFJzm16E"
      },
      "source": [
        "## **Quick Gradio DEMO (Toy Deployment)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio** is a Python library that makes it extremely easy to create a simple web UI (a \"demo\") for your machine learning model. It's perfect for sharing your work with others or for creating a quick prototype.\n",
        "\n",
        "The `gr.Interface` class is the core of Gradio. It wraps your Python function in a user interface. It has three main arguments:\n",
        "1.  `fn`: The function that you want to create a UI for. This function will take some input(s) and return some output(s).\n",
        "2.  `inputs`: Defines the type of input component to show in the UI. This could be a `gr.Textbox()`, `gr.Image()`, `gr.Slider()`, etc.\n",
        "3.  `outputs`: Defines the type of output component. This is often a `gr.Textbox()` or `gr.Label()`.\n",
        "\n",
        "When you call `iface.launch()`, Gradio starts a local web server and gives you a URL where you can interact with your model through the UI you just defined."
      ],
      "metadata": {
        "id": "DzhIXq63uGjp"
      },
      "id": "DzhIXq63uGjp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VBY_4xlrl4Y1",
      "metadata": {
        "id": "VBY_4xlrl4Y1"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m-PhoTSSl172",
      "metadata": {
        "id": "m-PhoTSSl172"
      },
      "outputs": [],
      "source": [
        "# Please refer to the Hugging Face documentation for detailed instructions.\n",
        "# Example using Gradio in Hugging Face Spaces\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def translate(text):\n",
        "    translator = pipeline('translation_en_to_fr', model='t5-small')\n",
        "    return translator(text)[0]['translation_text']\n",
        "\n",
        "iface = gr.Interface(fn=translate, inputs=\"text\", outputs=\"text\",title=\"English to French Translation\",)  # title of the app)\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2lUdq7wI7Yf"
      },
      "source": [
        "## **Exercise: Cheese Review Analysis System**\n",
        "\n",
        "In this exercise, you'll apply what you've learned about HuggingFace pipelines to analyze cheese reviews. You'll perform sentiment analysis and summarization on a collection of cheese reviews.\n"
      ],
      "id": "h2lUdq7wI7Yf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efBes8pKI7Yk"
      },
      "source": [
        "> 💡 <b><font color=\"#a51c30\">Optional Challenge:</font></b> Try to complete all the steps on your own, without referring to the scaffolding. Or create your own custom analysis system using the HuggingFace pipelines we've explored!"
      ],
      "id": "efBes8pKI7Yk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKR4ACwXI7Yo"
      },
      "source": [
        "---"
      ],
      "id": "wKR4ACwXI7Yo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSf7OEtyI7Yr"
      },
      "source": [
        "### **Part 1: Create Your Dataset**\n",
        "\n",
        "First, create a dataset of cheese reviews with both positive and negative examples. Feel free to add your own reviews to the list!\n"
      ],
      "id": "gSf7OEtyI7Yr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZjBaFBWI7Yu"
      },
      "outputs": [],
      "source": [
        "print(\"Loading cheese review dataset...\")\n",
        "\n",
        "# TODO: add some positive and negative reviews\n",
        "cheese_reviews = [\n",
        "    # Positive reviews\n",
        "    \"This aged cheddar has an incredible sharp taste with perfect crumbly texture. Absolutely divine!\",\n",
        "    \"The brie is wonderfully creamy and buttery, melts perfectly at room temperature.\",\n",
        "    \"Outstanding Parmigiano-Reggiano! The texture is perfectly granular with delightful crystallization and nutty flavor.\",\n",
        "\n",
        "    # Negative reviews\n",
        "    \"This mozzarella was rubbery and flavorless, very disappointing.\",\n",
        "    \"The blue cheese had an overwhelming ammonia smell and bitter aftertaste.\",\n",
        "    \"Terrible cheddar - the texture was waxy and artificial, tasted like plastic.\",\n",
        "\n",
        "    # Your review(s)\n",
        "]\n",
        "\n",
        "print(f\"Dataset created with {len(cheese_reviews)} reviews\\n\")"
      ],
      "id": "XZjBaFBWI7Yu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 2: Sentiment Analysis**\n",
        "\n",
        "Use the sentiment-analysis pipeline to classify each review."
      ],
      "metadata": {
        "id": "NF9h26x7BzGG"
      },
      "id": "NF9h26x7BzGG"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the classifier for sentiment analysis\n",
        "# Initialize the sentiment analysis pipeline using a pre-trained model.\n",
        "print(\"Performing sentiment analysis...\")\n",
        "classifier = ...\n",
        "print(\"Classifier ready.\")"
      ],
      "metadata": {
        "id": "jiDzFBFoB-CP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jiDzFBFoB-CP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, loop through each review, classify it, and store the results.\n"
      ],
      "metadata": {
        "id": "mTmpBGCJB-qU"
      },
      "id": "mTmpBGCJB-qU"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Get the classifier's result for each review\n",
        "# Create a list to store the results for each review.\n",
        "sentiment_results = []\n",
        "for review in cheese_reviews:\n",
        "    if review:\n",
        "        # The classifier returns a list, so we take the first element.\n",
        "        result = ...\n",
        "        sentiment_results.append({\n",
        "            'review': review,\n",
        "            'sentiment': result['label'],\n",
        "            'confidence': result['score']\n",
        "        })\n",
        "print(\"Collected sentiment for all reviews.\")"
      ],
      "metadata": {
        "id": "oV8Z8sALB_Ml"
      },
      "execution_count": null,
      "outputs": [],
      "id": "oV8Z8sALB_Ml"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the sentiment analysis results for verification."
      ],
      "metadata": {
        "id": "3X3_bKVzCRj4"
      },
      "id": "3X3_bKVzCRj4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results in a formatted way.\n",
        "print(\"=== Sentiment Analysis Results ===\")\n",
        "print(\"-\" * 50)\n",
        "for i, result in enumerate(sentiment_results, 1):\n",
        "    print(f\"\\nReview {i}:\")\n",
        "    print(f\"Text: {result['review'][:80]}...\")\n",
        "    print(f\"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.3f})\")"
      ],
      "metadata": {
        "id": "FY4FOLylCSZI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FY4FOLylCSZI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 3: Categorize Reviews by Aspect**\n",
        "\n",
        "Use a 'zero-shot-classification' pipeline to categorize reviews without needing a specially trained model."
      ],
      "metadata": {
        "id": "Q9jamL4ICSA9"
      },
      "id": "Q9jamL4ICSA9"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define the zero-shot classifier\n",
        "# Initialize a zero-shot classification pipeline.\n",
        "print(\"Initializing zero-shot classifier for aspect categorization...\")\n",
        "aspect_classifier = ...\n",
        "print(\"Classifier ready.\")"
      ],
      "metadata": {
        "id": "2uQ5RZ-gCeCT"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2uQ5RZ-gCeCT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create your own category labels for the classifier."
      ],
      "metadata": {
        "id": "OArhPt_EECol"
      },
      "id": "OArhPt_EECol"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create your own category labels\n",
        "# Define the categories we want to sort reviews into.\n",
        "candidate_labels = ['texture', ...]\n",
        "\n",
        "# Prepare a dictionary to hold the categorized reviews.\n",
        "categories = {label: [] for label in candidate_labels}\n",
        "print(\"Categories created.\")"
      ],
      "metadata": {
        "id": "1nZwtz7MEIXH"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1nZwtz7MEIXH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Categorize each review, allowing it to belong to multiple categories if the model is confident enough."
      ],
      "metadata": {
        "id": "9F-t15ujCSsA"
      },
      "id": "9F-t15ujCSsA"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO get the classification for each review\n",
        "print(\"Categorizing reviews by aspect using zero-shot classification...\")\n",
        "\n",
        "for review in cheese_reviews:\n",
        "    if review:\n",
        "        # The pipeline returns the label with the highest score by default.\n",
        "        result = ...\n",
        "\n",
        "        # The top-scoring label is the predicted category.\n",
        "        predicted_category = result['labels'][0]\n",
        "        categories[predicted_category].append(review)\n",
        "\n",
        "print(\"Categorization complete.\")"
      ],
      "metadata": {
        "id": "AvzNQdO5CTGr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AvzNQdO5CTGr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Display the categorized reviews."
      ],
      "metadata": {
        "id": "hLxDq4xMCwN0"
      },
      "id": "hLxDq4xMCwN0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the reviews sorted into their predicted categories.\n",
        "print(\"\\n=== Reviews by Category ===\")\n",
        "print(\"-\" * 50)\n",
        "for category, reviews in categories.items():\n",
        "    print(f\"\\n{category.upper()} ({len(reviews)} reviews):\")\n",
        "    # Print the first two reviews in each category for brevity.\n",
        "    for review in reviews[:2]:\n",
        "        print(f\"  - {review[:70]}...\")"
      ],
      "metadata": {
        "id": "tIIzFR6nCwp_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tIIzFR6nCwp_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 4: Separate Positive and Negative Reviews**\n",
        "\n",
        "Organize reviews by their sentiment for separate summarization."
      ],
      "metadata": {
        "id": "XmblClfnCw_W"
      },
      "id": "XmblClfnCw_W"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create separate lists for positive and negative reviews based on sentiment results.\n",
        "positive_reviews, negative_reviews = [], []\n",
        "for result in sentiment_results:\n",
        "    if result['sentiment'] == 'POSITIVE':\n",
        "        positive_reviews.append(result['review'])\n",
        "    else:\n",
        "        negative_reviews.append(result['review'])\n",
        "\n",
        "print(f\"\\nFound {len(positive_reviews)} positive reviews\")\n",
        "print(f\"Found {len(negative_reviews)} negative reviews\")"
      ],
      "metadata": {
        "id": "vi751868CxXR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vi751868CxXR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Part 5: Generate Summaries**\n",
        "\n",
        "Create summaries of the negative reviews, positive reviews, and each category using the summarization pipeline."
      ],
      "metadata": {
        "id": "vdo_7MdiCx47"
      },
      "id": "vdo_7MdiCx47"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the summarization pipeline."
      ],
      "metadata": {
        "id": "NTSAEOnfLA2x"
      },
      "id": "NTSAEOnfLA2x"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the summarizer\n",
        "# Initialize the summarization pipeline.\n",
        "print(\"Initializing summarizer...\")\n",
        "summarizer = ...\n",
        "print(\"Summarizer ready.\")"
      ],
      "metadata": {
        "id": "1C85vACeCyWz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1C85vACeCyWz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Combine the reviews into single blocks of text for summarization."
      ],
      "metadata": {
        "id": "LFc8lM1QDKue"
      },
      "id": "LFc8lM1QDKue"
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the lists of reviews into single strings.\n",
        "positive_text = \" \".join(positive_reviews)\n",
        "negative_text = \" \".join(negative_reviews)\n",
        "print(\"Texts combined.\")"
      ],
      "metadata": {
        "id": "dh-HSQ9vDLEa"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dh-HSQ9vDLEa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Generate a summary for the negative reviews."
      ],
      "metadata": {
        "id": "8HFP_XuADLgU"
      },
      "id": "8HFP_XuADLgU"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Get the summary of the negative reviews\n",
        "# Generate a summary if the text is long enough.\n",
        "if negative_text:\n",
        "    if len(negative_text.split()) > 20:\n",
        "        negative_summary = summarizer(..., max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n",
        "        print(\"\\n=== Summary of Negative Reviews ===\")\n",
        "        print(negative_summary)\n",
        "    else:\n",
        "        print(\"\\n=== Summary of Negative Reviews ===\")\n",
        "        print(\"Reviews too short for summarization.\")"
      ],
      "metadata": {
        "id": "DoSYUy2IDL92"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DoSYUy2IDL92"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Generate a summary for the positive reviews."
      ],
      "metadata": {
        "id": "2kqwgP8cDMYe"
      },
      "id": "2kqwgP8cDMYe"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Get the summary of the positive reviews\n",
        "# Generate a summary if the text is long enough.\n",
        "if positive_text:\n",
        "    if len(positive_text.split()) > 20:\n",
        "        positive_summary = summarizer(..., max_length=60, min_length=20, do_sample=False)[0]['summary_text']\n",
        "        print(\"\\n=== Summary of Positive Reviews ===\")\n",
        "        print(positive_summary)\n",
        "    else:\n",
        "        print(\"\\n=== Summary of Positive Reviews ===\")\n",
        "        print(\"Reviews too short for summarization.\")"
      ],
      "metadata": {
        "id": "19Fm-O_oDMuG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "19Fm-O_oDMuG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Finally, generate a summary for each category."
      ],
      "metadata": {
        "id": "f0nWzzdyDNCm"
      },
      "id": "f0nWzzdyDNCm"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Get the summary for each category\n",
        "print(\"\\n=== Summaries by Category ===\")\n",
        "for category, reviews in categories.items():\n",
        "    if reviews:\n",
        "        # Combine reviews for the current category into a single text.\n",
        "        category_text = \" \".join(reviews)\n",
        "        if len(category_text.split()) > 20:\n",
        "            summary = ...\n",
        "            print(f\"\\n--- Summary for '{category.upper()}' Reviews ---\")\n",
        "            print(summary)\n",
        "        else:\n",
        "            print(f\"\\n--- Summary for '{category.upper()}' Reviews ---\")\n",
        "            print(f\"Reviews for {category} are too short to summarize.\")"
      ],
      "metadata": {
        "id": "7NiTsFCjDNbd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7NiTsFCjDNbd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part 6: Combined Analysis Report**\n",
        "\n",
        "Create a final report that dynamically combines all the findings from the previous steps."
      ],
      "metadata": {
        "id": "pjMPOLA5DegN"
      },
      "id": "pjMPOLA5DegN"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== CHEESE REVIEW ANALYSIS REPORT ===\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# TODO: Create a comprehensive report including the total number of reviews analyzed, sentiment distribution, most discussed aspects, key positive points, and key negative points.\n",
        "\n",
        "# Your report code here\n"
      ],
      "metadata": {
        "id": "wJx2N5IRDe3h"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wJx2N5IRDe3h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "TvUePqulzn2Q"
      },
      "id": "TvUePqulzn2Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BONUS Challenge - 1 (Optional )** 🏆\n",
        "\n",
        "Try using different models for sentiment analysis and summarization. Compare the results and discuss which models work better for cheese reviews.\n"
      ],
      "metadata": {
        "id": "_1W5iSV8mcd8"
      },
      "id": "_1W5iSV8mcd8"
    },
    {
      "cell_type": "code",
      "source": [
        "alternative_models = {\n",
        "    'sentiment': ['cardiffnlp/twitter-roberta-base-sentiment'],\n",
        "    'summarization': ['facebook/bart-large-cnn']\n",
        "}\n",
        "\n",
        "# TODO: Try alternative models (optional) and compare the results\n",
        "# Your comparison code here (if attempting the bonus)\n"
      ],
      "metadata": {
        "id": "mTt9Aod0mdoc"
      },
      "id": "mTt9Aod0mdoc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BONUS Challenge - 2 (Optional)** 🏆\n",
        "\n",
        "**Build a Gradio App 💻**\n",
        "\n",
        "As a bonus, take your cheese review analyzer one step further and make it **interactive** with [Gradio](https://www.gradio.app/).\n",
        "\n",
        "### What to do:\n",
        "1. Add a **Gradio interface** with:\n",
        "   - A **textbox** where users can paste multiple reviews (one per line).\n",
        "   - A **button** to run the analysis.\n",
        "   - Outputs showing:\n",
        "     - The **sentiment** (positive/negative) of each review.\n",
        "     - The **category** (e.g., taste, texture, aroma, price).\n",
        "     - A short **positive summary**.\n",
        "     - A short **negative summary**.\n",
        "2. Launch the app and test it with your own reviews."
      ],
      "metadata": {
        "id": "_M00Xv6v-PQo"
      },
      "id": "_M00Xv6v-PQo"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoHapyrWjRJr"
      },
      "id": "zoHapyrWjRJr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "2Y_S38T0pA7N",
        "gSf7OEtyI7Yr",
        "NF9h26x7BzGG",
        "Q9jamL4ICSA9",
        "XmblClfnCw_W",
        "vdo_7MdiCx47",
        "pjMPOLA5DegN"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}